# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_core.ipynb.

# %% auto 0
__all__ = ['NoQueryError', 'Harvester', 'prepare_query', 'get_harvest', 'get_metadata']

# %% ../00_core.ipynb 2
import argparse
import datetime
import json
import os
import re
import time
from importlib.metadata import version
from pathlib import Path
from pprint import pprint
from urllib.parse import parse_qs, parse_qsl, urlparse

import arrow
import html2text
import pandas as pd
import requests_cache
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from requests.exceptions import HTTPError
from requests.packages.urllib3.util.retry import Retry
from tqdm.auto import tqdm
from trove_newspaper_images.articles import download_images
from trove_query_parser.parser import parse_query

# %% ../00_core.ipynb 4
class NoQueryError(Exception):
    """
    Exception triggered by empty query.
    """
    pass


class Harvester:
    """
    Harvest large quantities of digitised newspaper articles from Trove.

    Parameters:

    * `query_params` [required, dictionary of parameters]
    * `data_dir` [optional, directory for harvests, string]
    * `harvest_dir` [optional, directory for this harvest, string]
    * `text` [optional, save articles as text files, True or False]
    * `pdf` [optional, save articles as PDFs, True or False]
    * `image` [optional, save articles as images, True or False]
    * `include_linebreaks` [optional, include linebreaks in text files, True or False]
    * `max` [optional, maximum number of results, integer]
    """

    zoom = 3
    api_url = "https://api.trove.nla.gov.au/v2/result"

    def __init__(
        self,
        query_params,
        data_dir="data",
        harvest_dir=None,
        text=False,
        pdf=False,
        image=False,
        include_linebreaks=False,
        max=None,
    ):
        self.query_params = query_params
        self.data_dir = Path(data_dir)
        if harvest_dir:
            self.harvest_dir = Path(self.data_dir, harvest_dir)
        else:
            self.harvest_dir = Path(
                self.data_dir, arrow.utcnow().format("YYYYMMDDHHmmss")
            )
        self.s = self.initialise_cache()
        self.ndjson_file = Path(self.harvest_dir, "results.ndjson")
        # Deletes existing file in case of restart
        self.ndjson_file.unlink(missing_ok=True)
        self.pdf = pdf
        self.text = text
        self.image = image
        self.create_dirs()
        self.include_linebreaks = include_linebreaks
        self.harvested = 0
        self.start = "*"
        self.number = 100
        if max:
            self.maximum = max
        else:
            self._get_total()
        self.save_meta()

    def initialise_cache(self):
        cache_name = "-".join(self.harvest_dir.parts)
        s = requests_cache.CachedSession(cache_name)
        retries = Retry(
            total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504]
        )
        s.mount("http://", HTTPAdapter(max_retries=retries))
        s.mount("https://", HTTPAdapter(max_retries=retries))
        return s

    def delete_cache(self):
        cache_name = f"{'-'.join(self.harvest_dir.parts)}.sqlite"
        Path(cache_name).unlink(missing_ok=True)

    def create_dirs(self):
        self.harvest_dir.mkdir(exist_ok=True, parents=True)
        if self.pdf:
            Path(self.harvest_dir, "pdf").mkdir(exist_ok=True)
        if self.text:
            Path(self.harvest_dir, "text").mkdir(exist_ok=True)
        if self.image:
            Path(self.harvest_dir, "image").mkdir(exist_ok=True)

    def _get_total(self):
        try:
            params = self.query_params.copy()
        except AttributeError:
            raise NoQueryError
        else:
            params["n"] = 0
            response = self.s.get(self.api_url, params=params, timeout=30)
            response.raise_for_status()
            # print(response.url)
            try:
                results = response.json()
            except (AttributeError, ValueError):
                self.maximum = 0
            else:
                self.maximum = int(results["response"]["zone"][0]["records"]["total"])

    def log_query(self):
        """
        Do something with details of query -- ie log date?
        """
        pass

    def harvest(self):
        """
        Start the harvest and loop over the result set until finished.
        """
        if self.maximum > 0:
            params = self.query_params.copy()
            params["n"] = self.number
            with tqdm(total=self.maximum, unit="article") as pbar:
                pbar.update(self.harvested)
                while self.start and (self.harvested < self.maximum):
                    params["s"] = self.start
                    response = self.s.get(self.api_url, params=params, timeout=30)
                    response.raise_for_status()
                    # print(response.url)
                    try:
                        results = response.json()
                    except (AttributeError, ValueError):
                        # Log errors?
                        pass
                    else:
                        records = results["response"]["zone"][0]["records"]
                        self.process_results(records, pbar)
                        # pbar.update(len(records['article']))
                    if not response.from_cache:
                        time.sleep(0.2)
        # Add the number harvested to the metadata file
        self.update_meta()
        self.delete_cache()

    def save_meta(self):
        """
        Save the query metadata in a JSON file.
        Useful for documenting your harvest.
        """
        meta = {
            "query_parameters": self.query_params,
            "harvest_directory": str(self.harvest_dir),
            "max": self.maximum,
            "text": self.text,
            "pdf": self.pdf,
            "image": self.image,
            "include_linebreaks": self.include_linebreaks,
            "date_started": arrow.utcnow().isoformat(),
            "harvester": f"trove_newspaper_harvester v{version('trove_newspaper_harvester')}",
        }
        with Path(self.harvest_dir, "metadata.json").open("w") as meta_file:
            json.dump(meta, meta_file, indent=4)

    def update_meta(self):
        """
        Update the metadata file with the total harvested.
        """
        meta = get_metadata(self.harvest_dir)
        if meta:
            meta["harvested"] = self.harvested
        with Path(self.harvest_dir, "metadata.json").open("w") as meta_file:
            json.dump(meta, meta_file, indent=4)

    def create_page_url(self, url):
        if not pd.isnull(url):
            page_id = re.search(r"page\/(\d+)", url).group(1)
            return f"http://trove.nla.gov.au/newspaper/page/{page_id}"

    def save_csv(self):
        """
        Flatten and rename data in the ndjson file to save as CSV.
        """
        json_data = []
        with self.ndjson_file.open("r") as ndjson_file:
            for line in ndjson_file:
                json_data.append(json.loads(line.strip()))
        df = pd.json_normalize(json_data)
        df["page_url"] = df["trovePageUrl"].apply(self.create_page_url)
        df["images"] = df["images"].str.join("|")
        for part in [
            "edition",
            "supplement",
            "section",
            "lastCorrection.lastupdated",
            "snippet",
        ]:
            if part not in df.columns:
                df[part] = ""
        df = df[
            [
                "id",
                "heading",
                "date",
                "pageSequence",
                "title.id",
                "title.value",
                "category",
                "wordCount",
                "illustrated",
                "edition",
                "supplement",
                "section",
                "identifier",
                "page_url",
                "snippet",
                "relevance.score",
                "correctionCount",
                "lastCorrection.lastupdated",
                "tagCount",
                "commentCount",
                "listCount",
                "articleText",
                "pdf",
                "images",
            ]
        ]
        df = df.rename(
            columns={
                "id": "article_id",
                "heading": "title",
                "pageSequence": "page",
                "title.id": "newspaper_id",
                "title.value": "newspaper_title",
                "wordCount": "words",
                "correctionCount": "corrections",
                "lastCorrection.lastupdated": "last_corrected",
                "identifier": "url",
                "relevance.score": "relevance",
                "tagCount": "tags",
                "commentCount": "comments",
                "listCount": "lists",
                "articleText": "text",
            }
        )

        df.to_csv(Path(self.harvest_dir, "results.csv"), index=False)

    def make_filename(self, article):
        """
        Create a filename for a text file or PDF.
        For easy sorting/aggregation the filename has the format:
            PUBLICATIONDATE-NEWSPAPERID-ARTICLEID
        """
        # If the article object doesn't have basic info like date, there's something wrong
        # Don't try and save files if that's the case
        try:
            date = article["date"]
        except KeyError:
            return None
        date = date.replace("-", "")
        newspaper_id = article["title"]["id"]
        article_id = article["id"]
        return f"{date}-{newspaper_id}-{article_id}"

    def ping_pdf(self, ping_url):
        """
        Check to see if a PDF is ready for download.
        If a 200 status code is received, return True.
        """
        ready = False
        # req = Request(ping_url)
        try:
            # urlopen(req)
            with self.s.cache_disabled():
                response = self.s.get(ping_url, timeout=30)
            response.raise_for_status()
        except HTTPError:
            if response.status_code == 423:
                ready = False
            else:
                raise
        else:
            ready = True
        return ready

    def get_pdf_url(self, article_id, zoom=3):
        """
        Download the PDF version of an article.
        These can take a while to generate, so we need to ping the server to see if it's ready before we download.
        """
        pdf_url = None
        # Ask for the PDF to be created
        prep_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}/level/{}/prep".format(
            article_id, zoom
        )
        response = self.s.get(prep_url)
        # Get the hash
        prep_id = response.text
        # Url to check if the PDF is ready
        ping_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.ping?followup={}".format(
            article_id, zoom, prep_id
        )
        tries = 0
        ready = False
        time.sleep(1)  # Give some time to generate pdf
        # Are you ready yet?
        while ready is False and tries < 5:
            ready = self.ping_pdf(ping_url)
            if not ready:
                tries += 1
                time.sleep(2)
        # Download if ready
        if ready:
            pdf_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.pdf?followup={}".format(
                article_id, zoom, prep_id
            )
        return pdf_url

    def get_aww_text(self, article_id):
        # Download text using the link from the web interface
        url = f"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{article_id}.txt"
        response = self.s.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "lxml")
            # Remove the header
            soup.find("p").decompose()
            soup.find("hr").decompose()
            return str(soup)

    def save_text(self, article):
        text_filename = self.make_filename(article)
        if text_filename:
            text_file = Path(self.harvest_dir, "text", f"{text_filename}.txt")
            if not text_file.exists():
                html_text = article.get("articleText")
                if not html_text:
                    # If the text isn't in the API response (as with AWW), download separately
                    html_text = self.get_aww_text(article["id"])
                if html_text:
                    # Convert html to plain text
                    text = html2text.html2text(html_text)
                    if self.include_linebreaks == False:
                        text = re.sub("\s+", " ", text)

                    with open(text_file, "wb") as text_output:
                        text_output.write(text.encode("utf-8"))
                else:
                    return ""
            # Removes the output_dir from path
            return text_file.relative_to(*text_file.parts[:2])
        else:
            return ""

    def save_pdf(self, article):
        pdf_filename = self.make_filename(article)
        if pdf_filename:
            pdf_file = Path(self.harvest_dir, "pdf", f"{pdf_filename}.pdf")
            if not pdf_file.exists():
                pdf_url = self.get_pdf_url(article["id"])
                if pdf_url:
                    response = self.s.get(pdf_url)
                    pdf_file.write_bytes(response.content)
                    # Removes the output_dir from path
                else:
                    return ""
            return pdf_file.relative_to(*pdf_file.parts[:2])
        else:
            return ""

    def process_results(self, records, pbar):
        """
        Processes a page full of results.
        """
        rows = []
        try:
            articles = records["article"]
        except KeyError:
            raise
        else:
            with self.ndjson_file.open("a") as ndjson_file:
                for article in articles:
                    if self.harvested >= self.maximum:
                        break
                    article_id = article["id"]
                    # rows.append(self.prepare_row(article))

                    if self.pdf:
                        pdf_file = self.save_pdf(article)
                        article["pdf"] = str(pdf_file)
                    else:
                        article["pdf"] = ""
                    if self.text:
                        text_file = self.save_text(article)
                        article["articleText"] = str(text_file)
                    else:
                        article["articleText"] = ""
                    if self.image:
                        images = download_images(
                            article_id, output_dir=Path(self.harvest_dir, "image")
                        )
                        images = [str(Path("image", i)) for i in images]
                        article["images"] = images
                    else:
                        article["images"] = []
                    ndjson_file.write(json.dumps(article) + "\n")
                    pbar.update(1)
                    # Update the number harvested
                    self.harvested += 1
            # Get the nextStart token
            try:
                self.start = records["nextStart"]
            except KeyError:
                self.start = None
            # print('Harvested: {}'.format(self.harvested))


def prepare_query(query, api_key, text=False):
    """
    Converts a Trove search url into a set of parameters ready for harvesting.

    Parameters:

    * `query` [required, search url from Trove web interface or API, string]
    * `api_key` [required, Trove API key, string]
    * `text` [optional, save text files, True or False]

    Returns:

    * a dictionary of parameters
    """
    if query:
        if text and "articleText" not in query:
            # If text is set to True, make sure the query is getting the article text
            # Adding it here rather than to the params dict to avoid overwriting any existing include values
            query += "&include=articleText"
        if "api.trove.nla.gov.au" in query:
            # If it's an API url, no further processing of parameters needed
            parsed_url = urlparse(query)
            new_params = parse_qs(parsed_url.query)
        else:
            # These params can be accepted as is.
            new_params = parse_query(query)
        new_params["key"] = api_key
        new_params["encoding"] = "json"
        new_params["reclevel"] = "full"
        new_params["bulkHarvest"] = "true"
        # The query parser defaults to 'newspaper,gazette' if no zone is set.
        # But multiple zones won't work with bulkHarvest, so set to 'newspaper'.
        if new_params["zone"] == "newspaper,gazette":
            new_params["zone"] = "newspaper"
        # return '{}?{}'.format('https://api.trove.nla.gov.au/v2/result', urlencode(new_params, doseq=True))
        return new_params


def get_harvest(data_dir="data", harvest_dir=None):
    """
    Get the path to a harvest.
    If data_dir and harvest_dir are not supplied, this will return the most recent harvest in the 'data' directory.

    Parameters:

    * `data_dir` [optional, directory for harvests, string]
    * `harvest_dir` [optional, directory for this harvest, string]

    Returns:

    * a pathlib.Path object pointing to the harvest directory
    """
    if harvest_dir:
        harvest = Path(data_dir, harvest_dir)
    else:
        harvests = Path("data").glob("*")
        harvests = sorted([d for d in harvests if d.is_dir()])
        harvest = Path(harvests[-1])
    return harvest


def get_metadata(harvest):
    """
    Get the query metadata from a harvest directory.

    Parameters:

    * `harvest` [required, path to harvest, string or pathlib.Path]

    Returns:

    * metadata dictionary
    """
    try:
        with Path(harvest, "metadata.json").open("r") as meta_file:
            meta = json.load(meta_file)
    except IOError:
        print("No harvest!")
        meta = None
    return meta
