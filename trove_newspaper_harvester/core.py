# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_core.ipynb.

# %% auto 0
__all__ = ['NoQueryError', 'Harvester', 'prepare_query', 'get_harvest', 'get_config', 'get_crate']

# %% ../00_core.ipynb 2
import argparse
import csv
import datetime
import json
import os
import re
import time
from importlib.metadata import version
from pathlib import Path
from pprint import pprint
from urllib.parse import parse_qs, parse_qsl, urlparse

import arrow
import html2text
import requests_cache
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from requests.exceptions import HTTPError
from requests.packages.urllib3.util.retry import Retry
from rocrate.rocrate import ROCrate
from rocrate.model.contextentity import ContextEntity
from rocrate.model.person import Person
from tqdm.auto import tqdm
from trove_newspaper_images.articles import download_images
from trove_query_parser.parser import parse_query

# %% ../00_core.ipynb 4
class NoQueryError(Exception):
    """
    Exception triggered by empty query.
    """
    pass


class Harvester:
    """
    Harvest large quantities of digitised newspaper articles from Trove. Note that you must supply either `query_params` and `key` or `config_file`.

    Parameters:

    * `query_params` [optional, dictionary of parameters]
    * `key` [optional, Trove API key]
    * `config_file` [optional, path to a config file]
    * `data_dir` [optional, directory for harvests, string]
    * `harvest_dir` [optional, directory for this harvest, string]
    * `text` [optional, save articles as text files, True or False]
    * `pdf` [optional, save articles as PDFs, True or False]
    * `image` [optional, save articles as images, True or False]
    * `include_linebreaks` [optional, include linebreaks in text files, True or False]
    * `maximum` [optional, maximum number of results, integer]
    """

    zoom = 3
    api_url = "https://api.trove.nla.gov.au/v3/result"

    def __init__(
        self,
        query_params=None,
        key=None,
        data_dir="data",
        harvest_dir=None,
        config_file=None,
        text=False,
        pdf=False,
        image=False,
        include_linebreaks=False,
        maximum=None,
    ):
        if not (query_params and key) and not config_file:
            raise NoQueryError
            
        if config_file:
            config = json.loads(Path(config_file).read_text())
            self.query_params = config["query_params"]
            self.key = config["key"]
            self.pdf = config["pdf"]
            self.text = config["text"]
            self.image = config["image"]
            self.include_linebreaks = config["include_linebreaks"]
            self.maximum = config["maximum"]
        else:
            self.query_params = query_params
            self.key = key
            self.pdf = pdf
            self.text = text
            self.image = image
            self.include_linebreaks = include_linebreaks
            self.maximum = maximum
        if self.text:
            try:
                self.query_params["include"].append("articleText")
            except KeyError:
                self.query_params["include"] = ["articleText"]
        self.data_dir = Path(data_dir)
        if harvest_dir:
            self.harvest_dir = Path(self.data_dir, harvest_dir)
        else:
            self.harvest_dir = Path(
                self.data_dir, arrow.utcnow().format("YYYYMMDDHHmmss")
            )
        self.s = self.initialise_cache()
        self.ndjson_file = Path(self.harvest_dir, "results.ndjson")
        # Deletes existing file in case of restart
        self.ndjson_file.unlink(missing_ok=True)
        self.create_dirs()
        self.harvested = 0
        self.start = "*"
        self.number = 100
        if self.maximum:
            self.total = self.maximum
        else:
            self.total = self._get_total()
        self.save_config()
        self.create_crate()

    def initialise_cache(self):
        cache_name = "-".join(self.harvest_dir.parts)
        s = requests_cache.CachedSession(cache_name)
        retries = Retry(
            total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504]
        )
        s.mount("http://", HTTPAdapter(max_retries=retries))
        s.mount("https://", HTTPAdapter(max_retries=retries))
        return s

    def delete_cache(self):
        cache_name = f"{'-'.join(self.harvest_dir.parts)}.sqlite"
        Path(cache_name).unlink(missing_ok=True)

    def create_dirs(self):
        self.harvest_dir.mkdir(exist_ok=True, parents=True)
        if self.pdf:
            Path(self.harvest_dir, "pdf").mkdir(exist_ok=True)
        if self.text:
            Path(self.harvest_dir, "text").mkdir(exist_ok=True)
        if self.image:
            Path(self.harvest_dir, "image").mkdir(exist_ok=True)

    def _get_total(self):
        try:
            params = self.query_params.copy()
        except AttributeError:
            raise NoQueryError
        else:
            params["n"] = 0
            response = self.s.get(self.api_url, params=params, headers={"X-API-KEY": self.key}, timeout=30)
            response.raise_for_status()
            # print(response.url)
            try:
                results = response.json()
            except (AttributeError, ValueError):
                return 0
            else:
                return int(results["category"][0]["records"]["total"])

    def log_query(self):
        """
        Do something with details of query -- ie log date?
        """
        pass

    def harvest(self):
        """
        Start the harvest and loop over the result set until finished.
        """
        if self.total > 0:
            params = self.query_params.copy()
            params["n"] = self.number
            with tqdm(total=self.total, unit="article") as pbar:
                pbar.update(self.harvested)
                while self.start and (self.harvested < self.total):
                    params["s"] = self.start
                    response = self.s.get(self.api_url, params=params, headers={"X-API-KEY": self.key}, timeout=30)
                    response.raise_for_status()
                    # print(response.url)
                    try:
                        results = response.json()
                    except (AttributeError, ValueError):
                        # Log errors?
                        pass
                    else:
                        records = results["category"][0]["records"]
                        self.process_results(records, pbar)
                        # pbar.update(len(records['article']))
        # Add the number harvested to the metadata file
        self.update_crate()
        self.delete_cache()
        
    def create_crate(self):
        crate = ROCrate()
        
        # Add CreateAction with datetime started & instrument & object pointing to config
        harvest_properties = {
            "@type": "CreateAction",
            "name": "Run of harvester",
            "startDate": arrow.now().isoformat(),
            "instrument": "https://github.com/wragge/trove-newspaper-harvester",
            "object": "harvester_config.json",
            "actionStatus": {"@id": "http://schema.org/ActiveActionStatus"}
        }
        crate.add(ContextEntity(crate, "#harvester_run", properties=harvest_properties))
        
        # Add link to action from root
        crate.update_jsonld(
            {
                "@id": "./",
                "name": f"Dataset of digitised newspaper articles harvested from Trove on {arrow.now().format('D MMMM YYYY')}",
                "description": f"This dataset of digitised newspaper articles from Trove was created using the Trove Newspaper Harvester. Details of the search query used to generate this dataset can be found in the harvester_config.json file.",
                "mainEntity": {"@id": "#harvester_run"}
            }
        )
        
        # Add harvester as software
        harvester_properties = {
            "@type": "SoftwareApplication",
            "name": "Trove Newspaper and Gazette Harvester",
            "description": "The Trove Newspaper (& Gazette) Harvester makes it easy to download large quantities of digitised articles from Troveâ€™s newspapers and gazettes.",
            "documentation": "https://wragge.github.io/trove-newspaper-harvester/",
            "url": "https://github.com/wragge/trove-newspaper-harvester",
            "softwareVersion": version('trove_newspaper_harvester')
        }
        
        crate.add(ContextEntity(crate, "https://github.com/wragge/trove-newspaper-harvester", properties=harvester_properties))
        
        # Add config file
        config_properties = {
            "@type": "File",
            "name": "Trove Newspaper Harvester configuration file",
            "encodingFormat": "application/json"
        }
        
        crate.add_file(Path(self.harvest_dir, "harvester_config.json"), properties=config_properties)
        
        # Add licences
        # For newspaper metadata
        nkc_properties = {
            "@type": "CreativeWork",
            "url": "http://rightsstatements.org/vocab/NKC/1.0/",
            "name": "No Known Copyright",
            "description": "The organization that has made the Item available reasonably believes that the Item is not restricted by copyright or related rights, but a conclusive determination could not be made."
        }
        
        # For text, pdfs, images
        cne_properties = {
            "@type": "CreativeWork",
            "url": "http://rightsstatements.org/vocab/CNE/1.0/",
            "name": "Copyright Not Evaluated",
            "description": "The copyright and related rights status of this Item has not been evaluated."
        }
        
        # For crate metadata
        cc_properties = {
            "name": "CC0 Public Domain Dedication",
            "@type": "CreativeWork",
            "url": "https://creativecommons.org/publicdomain/zero/1.0/"
        }
        
        for licence in [nkc_properties, cne_properties, cc_properties]:
            crate.add(ContextEntity(crate, licence["url"], properties=licence))
        
        # Add licence to metadata
        crate.update_jsonld(
            {
                "@id": "ro-crate-metadata.json",
                "license": {"@id": "https://creativecommons.org/publicdomain/zero/1.0/"}
            }
        )
        
        crate.write(self.harvest_dir)

    def save_config(self):
        """
        Save the harvester config in a JSON file.
        Useful for documenting your harvest.
        """
        config = {
            "query_params": self.query_params,
            "key": self.key,
            "full_harvest_dir": str(self.harvest_dir),
            "maximum": self.maximum,
            "text": self.text,
            "pdf": self.pdf,
            "image": self.image,
            "include_linebreaks": self.include_linebreaks,
            #"date_started": arrow.utcnow().isoformat(),
            #"harvester": f"trove_newspaper_harvester v{version('trove_newspaper_harvester')}",
        }
        with Path(self.harvest_dir, "harvester_config.json").open("w") as config_file:
            json.dump(config, config_file, indent=4)

    def update_crate(self):
        """
        Update the RO-Crate file with the total harvested, end date, and files.
        """
        crate = ROCrate(source=self.harvest_dir)
        
        finished_date = arrow.now().isoformat()
        
        run_update = {
            "@id": "#harvester_run",
            "endDate": finished_date
        }
        
        if self.harvested > 0:
            run_update["actionStatus"] = {"@id": "http://schema.org/CompletedActionStatus"}
            run_update["result"] = [{"@id": "results.ndjson"}]
            
            ndjson_properties = {
                "@type": ["File", "Dataset"],
                "name": "Metadata of harvested articles in NDJSON format",
                "dateCreated": finished_date,
                "encodingFormat": "application/x-ndjson",
                "size": self.harvested,
                "contentSize": Path(self.harvest_dir, "results.ndjson").stat().st_size,
                "license": {"@id": "http://rightsstatements.org/vocab/NKC/1.0/"}
            }

            crate.add_file(Path(self.harvest_dir, "results.ndjson"), properties=ndjson_properties)

            if self.text:
                run_update["result"].append({"@id": "text"})
                text_properties = {
                    "@type": ["File", "Dataset"],
                    "name": "Text files harvested from articles",
                    "description": "There is one text file per article. The file titles include basic article metadata â€“ the date of the article, the id number of the newspaper, and the id number of the article.",
                    "dateCreated": finished_date,
                    "size": len(list(Path(self.harvest_dir, "text").glob('*.txt'))),
                    "license": {"@id": "http://rightsstatements.org/vocab/CNE/1.0/"}
                }
                crate.add_file(Path(self.harvest_dir, "text"), properties=text_properties)

            if self.pdf:
                run_update["result"].append({"@id": "pdf"})
                pdf_properties = {
                    "@type": ["File", "Dataset"],
                    "name": "PDF files of harvested articles",
                    "description": "There is one PDF file per article. The file titles include basic article metadata â€“ the date of the article, the id number of the newspaper, and the id number of the article.",
                    "dateCreated": finished_date,
                    "size": len(list(Path(self.harvest_dir, "pdf").glob('*.pdf'))),
                    "license": {"@id": "http://rightsstatements.org/vocab/CNE/1.0/"}
                }
                crate.add_file(Path(self.harvest_dir, "pdf"), properties=pdf_properties)

            if self.image:
                run_update["result"].append({"@id": "image"})
                image_properties = {
                    "@type": ["File", "Dataset"],
                    "name": "Images of harvested articles",
                    "description": "There can be multiple image files per article if the article was split over multiple pages. The file titles include basic article metadata â€“ the date of the article, the id number of the newspaper, the id number of the article, and the id number of the page.",
                    "dateCreated": finished_date,
                    "size": len(list(Path(self.harvest_dir, "image").glob('*.jpg'))),
                    "license": {"@id": "http://rightsstatements.org/vocab/CNE/1.0/"}
                }
                crate.add_file(Path(self.harvest_dir, "image"), properties=image_properties)
        else:
            run_update["actionStatus"] = {"@id": "http://schema.org/FailedActionStatus"}
                
        crate.update_jsonld(run_update)   
        crate.write(self.harvest_dir)
        
    def add_csv_to_crate(self):
        """
        Update the RO-Crate file with the total harvested, end date, and files.
        """
        crate = ROCrate(source=self.harvest_dir)
        
        csv_properties = {
            "@type": ["File", "Dataset"],
            "name": "Metadata of harvested articles in CSV format",
            "dateCreated": arrow.now().isoformat(),
            "encodingFormat": "text/csv",
            "size": self.harvested,
            "contentSize": Path(self.harvest_dir, "results.csv").stat().st_size,
            "license": {"@id": "http://rightsstatements.org/vocab/NKC/1.0/"}
        }
        
        crate.add_file(Path(self.harvest_dir, "results.csv"), properties=csv_properties)
        
        crate.get("#harvester_run").append_to("result", {"@id": "results.csv"})

        crate.write(self.harvest_dir)
        
    def remove_ndjson_from_crate(self):
        crate = ROCrate(source=self.harvest_dir)
        crate.delete("results.ndjson")
        crate.write(self.harvest_dir)
        outputs = crate.get("#harvester_run").properties()["result"]
        new_outputs = [o for o in outputs if o != {"@id": "results.ndjson"}]
        crate.update_jsonld({"@id": "#harvester_run", "result": new_outputs})
        crate.write(self.harvest_dir)

    def save_csv(self):
        """
        Flatten and rename data in the ndjson file to save as CSV.
        """
        with Path(self.harvest_dir, "results.csv").open('w') as csvfile:
            columns = ['article_id', 'title', 'date', 'page', 'newspaper_id', 'newspaper_title', 'category', 'words', 'illustrated', 'edition', 'supplement', 'section', 'url', 'page_url', 'snippet', 'relevance', 'status', 'corrections', 'last_corrected', 'tags', 'comments', 'lists', 'text', 'pdf', 'images']
            writer = csv.DictWriter(csvfile, fieldnames=columns)
            writer.writeheader()
            with self.ndjson_file.open("r") as ndjson_file:
                for line in ndjson_file:
                    data = json.loads(line.strip())
                    row = {
                        "article_id": data["id"],
                        "title": data["heading"],
                        "date": data["date"],
                        "page": data["pageSequence"],
                        "newspaper_id": data["title"]["id"],
                        "newspaper_title": data["title"].get("title", ""),
                        "category": data["category"],
                        "words": data["wordCount"],
                        "illustrated": data["illustrated"],
                        "edition": data.get("edition", ""),
                        "supplement": data.get("supplement", ""),
                        "section": data.get("section", ""),
                        "url": data.get("identifier", ""),
                        "page_url": data.get("trovePageUrl", ""),
                        "snippet": data.get("snippet", ""),
                        "relevance": data.get("relevance", {}).get("score", ""),
                        "status": data.get("status", ""),
                        "corrections": data.get("correctionCount", 0),
                        "last_corrected": data.get("lastCorrection", {}).get("lastupdated", ""),
                        "tags": data.get("tagCount", 0),
                        "comments": data.get("commentCount", 0),
                        "lists": data.get("listCount", 0),
                        "text": data["articleText"],
                        "pdf": data["pdf"],
                        "images": "|".join(data["images"])
                    }
                    writer.writerow(row)
        self.add_csv_to_crate()

    def make_filename(self, article):
        """
        Create a filename for a text file or PDF.
        For easy sorting/aggregation the filename has the format:
            PUBLICATIONDATE-NEWSPAPERID-ARTICLEID
        """
        # If the article object doesn't have basic info like date, there's something wrong
        # Don't try and save files if that's the case
        try:
            date = article["date"]
        except KeyError:
            return None
        date = date.replace("-", "")
        newspaper_id = article["title"]["id"]
        article_id = article["id"]
        return f"{date}-{newspaper_id}-{article_id}"

    def ping_pdf(self, ping_url):
        """
        Check to see if a PDF is ready for download.
        If a 200 status code is received, return True.
        """
        ready = False
        # req = Request(ping_url)
        try:
            # urlopen(req)
            with self.s.cache_disabled():
                response = self.s.get(ping_url, timeout=30)
            response.raise_for_status()
        except HTTPError:
            if response.status_code == 423:
                ready = False
            else:
                raise
        else:
            ready = True
        return ready

    def get_pdf_url(self, article_id, zoom=3):
        """
        Download the PDF version of an article.
        These can take a while to generate, so we need to ping the server to see if it's ready before we download.
        """
        pdf_url = None
        # Ask for the PDF to be created
        prep_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}/level/{}/prep".format(
            article_id, zoom
        )
        response = self.s.get(prep_url)
        # Get the hash
        prep_id = response.text
        # Url to check if the PDF is ready
        ping_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.ping?followup={}".format(
            article_id, zoom, prep_id
        )
        tries = 0
        ready = False
        time.sleep(1)  # Give some time to generate pdf
        # Are you ready yet?
        while ready is False and tries < 5:
            ready = self.ping_pdf(ping_url)
            if not ready:
                tries += 1
                time.sleep(2)
        # Download if ready
        if ready:
            pdf_url = "https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.pdf?followup={}".format(
                article_id, zoom, prep_id
            )
        return pdf_url

    def get_aww_text(self, article_id):
        # Download text using the link from the web interface
        url = f"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{article_id}.txt"
        response = self.s.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "lxml")
            # Remove the header
            soup.find("p").decompose()
            soup.find("hr").decompose()
            return str(soup)

    def save_text(self, article):
        text_filename = self.make_filename(article)
        if text_filename:
            text_file = Path(self.harvest_dir, "text", f"{text_filename}.txt")
            if not text_file.exists():
                html_text = article.get("articleText")
                if not html_text:
                    # If the text isn't in the API response (as with AWW), download separately
                    html_text = self.get_aww_text(article["id"])
                if html_text:
                    # Convert html to plain text
                    text = html2text.html2text(html_text)
                    if self.include_linebreaks == False:
                        text = re.sub("\s+", " ", text)

                    with open(text_file, "wb") as text_output:
                        text_output.write(text.encode("utf-8"))
                else:
                    return ""
            # Removes the output_dir from path
            return text_file.relative_to(*text_file.parts[:2])
        else:
            return ""

    def save_pdf(self, article):
        pdf_filename = self.make_filename(article)
        if pdf_filename:
            pdf_file = Path(self.harvest_dir, "pdf", f"{pdf_filename}.pdf")
            if not pdf_file.exists():
                pdf_url = self.get_pdf_url(article["id"])
                if pdf_url:
                    response = self.s.get(pdf_url)
                    pdf_file.write_bytes(response.content)
                    # Removes the output_dir from path
                else:
                    return ""
            return pdf_file.relative_to(*pdf_file.parts[:2])
        else:
            return ""

    def process_results(self, records, pbar):
        """
        Processes a page full of results.
        """
        rows = []
        try:
            articles = records["article"]
        except KeyError:
            raise
        else:
            with self.ndjson_file.open("a") as ndjson_file:
                for article in articles:
                    if self.harvested >= self.total:
                        break
                    article_id = article["id"]
                    # rows.append(self.prepare_row(article))

                    if self.pdf:
                        pdf_file = self.save_pdf(article)
                        article["pdf"] = str(pdf_file)
                    else:
                        article["pdf"] = ""
                    if self.text:
                        text_file = self.save_text(article)
                        article["articleText"] = str(text_file)
                    else:
                        article["articleText"] = ""
                    if self.image:
                        images = download_images(
                            article_id, output_dir=Path(self.harvest_dir, "image")
                        )
                        images = [str(Path("image", i)) for i in images]
                        article["images"] = images
                    else:
                        article["images"] = []
                    ndjson_file.write(json.dumps(article) + "\n")
                    pbar.update(1)
                    # Update the number harvested
                    self.harvested += 1
            # Get the nextStart token
            try:
                self.start = records["nextStart"]
            except KeyError:
                self.start = None
            # print('Harvested: {}'.format(self.harvested))


def prepare_query(query):
    """
    Converts a Trove search url into a set of parameters ready for harvesting.

    Parameters:

    * `query` [required, search url from Trove web interface or API, string]

    Returns:

    * a dictionary of parameters
    """
    if query:
        #if text and "articleText" not in query:
            # If text is set to True, make sure the query is getting the article text
            # Adding it here rather than to the params dict to avoid overwriting any existing include values
            #query += "&include=articleText"
        if "api.trove.nla.gov.au" in query:
            # If it's an API url, no further processing of parameters needed
            parsed_url = urlparse(query)
            new_params = parse_qs(parsed_url.query)
        else:
            # These params can be accepted as is.
            new_params = parse_query(query, 3)
        new_params["encoding"] = "json"
        new_params["reclevel"] = "full"
        new_params["bulkHarvest"] = "true"
        
        # The query parser defaults to 'newspaper,gazette' if no zone is set.
        # But multiple zones won't work with bulkHarvest, so set to 'newspaper'.
        #if new_params["zone"] == "newspaper,gazette":
        #    new_params["zone"] = "newspaper"
        # return '{}?{}'.format('https://api.trove.nla.gov.au/v2/result', urlencode(new_params, doseq=True))
        return new_params


def get_harvest(data_dir="data", harvest_dir=None):
    """
    Get the path to a harvest.
    If data_dir and harvest_dir are not supplied, this will return the most recent harvest in the 'data' directory.

    Parameters:

    * `data_dir` [optional, directory for harvests, string]
    * `harvest_dir` [optional, directory for this harvest, string]

    Returns:

    * a pathlib.Path object pointing to the harvest directory
    """
    if harvest_dir:
        harvest = Path(data_dir, harvest_dir)
    else:
        harvests = Path("data").glob("*")
        harvests = sorted([d for d in harvests if d.is_dir()])
        harvest = Path(harvests[-1])
    return harvest


def get_config(harvest):
    """
    Get the query config parameters from a harvest directory.

    Parameters:

    * `harvest` [required, path to harvest, string or pathlib.Path]

    Returns:

    * config dictionary
    """
    try:
        with Path(harvest, "harvester_config.json").open("r") as config_file:
            config = json.load(config_file)
    except IOError:
        print("No harvest!")
        config = None
    return config

def get_crate(harvest):
    """
    Get the RO-Crate metadata file from a harvest directory.
    
     Parameters:

    * `harvest` [required, path to harvest, string or pathlib.Path]

    Returns:

    * ROCrate object
    """
    return ROCrate(source=harvest)
