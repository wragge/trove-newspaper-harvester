{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> a harvester for downloading large numbers of digitised newspaper articles from Trove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from importlib.metadata import version\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from urllib.parse import parse_qs, parse_qsl, urlparse\n",
    "\n",
    "import arrow\n",
    "import html2text\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import HTTPError\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from rocrate.rocrate import ROCrate\n",
    "from rocrate.model.contextentity import ContextEntity\n",
    "from rocrate.model.person import Person\n",
    "from tqdm.auto import tqdm\n",
    "from trove_newspaper_images.articles import download_images\n",
    "from trove_query_parser.parser import parse_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import ExceptionExpected\n",
    "import pandas as pd\n",
    "\n",
    "# Load variables from the .env file if it exists\n",
    "# Use %%capture to suppress messages\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class NoQueryError(Exception):\n",
    "    \"\"\"\n",
    "    Exception triggered by empty query.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Harvester:\n",
    "    \"\"\"\n",
    "    Harvest large quantities of digitised newspaper articles from Trove. Note that you must supply either `query_params` and `key` or `config_file`.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * `query_params` [optional, dictionary of parameters]\n",
    "    * `key` [optional, Trove API key]\n",
    "    * `config_file` [optional, path to a config file]\n",
    "    * `data_dir` [optional, directory for harvests, string]\n",
    "    * `harvest_dir` [optional, directory for this harvest, string]\n",
    "    * `text` [optional, save articles as text files, True or False]\n",
    "    * `pdf` [optional, save articles as PDFs, True or False]\n",
    "    * `image` [optional, save articles as images, True or False]\n",
    "    * `include_linebreaks` [optional, include linebreaks in text files, True or False]\n",
    "    * `maximum` [optional, maximum number of results, integer]\n",
    "    \"\"\"\n",
    "\n",
    "    zoom = 3\n",
    "    api_url = \"https://api.trove.nla.gov.au/v3/result\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_params=None,\n",
    "        key=None,\n",
    "        data_dir=\"data\",\n",
    "        harvest_dir=None,\n",
    "        config_file=None,\n",
    "        text=False,\n",
    "        pdf=False,\n",
    "        image=False,\n",
    "        include_linebreaks=False,\n",
    "        maximum=None,\n",
    "    ):\n",
    "        if not (query_params and key) and not config_file:\n",
    "            raise NoQueryError\n",
    "            \n",
    "        if config_file:\n",
    "            config = json.loads(Path(config_file).read_text())\n",
    "            self.query_params = config[\"query_params\"]\n",
    "            self.key = config[\"key\"]\n",
    "            self.pdf = config[\"pdf\"]\n",
    "            self.text = config[\"text\"]\n",
    "            self.image = config[\"image\"]\n",
    "            self.include_linebreaks = config[\"include_linebreaks\"]\n",
    "            self.maximum = config[\"maximum\"]\n",
    "        else:\n",
    "            self.query_params = query_params\n",
    "            self.key = key\n",
    "            self.pdf = pdf\n",
    "            self.text = text\n",
    "            self.image = image\n",
    "            self.include_linebreaks = include_linebreaks\n",
    "            self.maximum = maximum\n",
    "        if self.text:\n",
    "            try:\n",
    "                self.query_params[\"include\"].append(\"articleText\")\n",
    "            except KeyError:\n",
    "                self.query_params[\"include\"] = [\"articleText\"]\n",
    "        self.data_dir = Path(data_dir)\n",
    "        if harvest_dir:\n",
    "            self.harvest_dir = Path(self.data_dir, harvest_dir)\n",
    "        else:\n",
    "            self.harvest_dir = Path(\n",
    "                self.data_dir, arrow.utcnow().format(\"YYYYMMDDHHmmss\")\n",
    "            )\n",
    "        self.s = self.initialise_cache()\n",
    "        self.ndjson_file = Path(self.harvest_dir, \"results.ndjson\")\n",
    "        # Deletes existing file in case of restart\n",
    "        self.ndjson_file.unlink(missing_ok=True)\n",
    "        self.create_dirs()\n",
    "        self.harvested = 0\n",
    "        self.start = \"*\"\n",
    "        self.number = 100\n",
    "        if self.maximum:\n",
    "            self.total = self.maximum\n",
    "        else:\n",
    "            self.total = self._get_total()\n",
    "        self.save_config()\n",
    "        self.create_crate()\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        cache_name = \"-\".join(self.harvest_dir.parts)\n",
    "        s = requests_cache.CachedSession(cache_name)\n",
    "        retries = Retry(\n",
    "            total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return s\n",
    "\n",
    "    def delete_cache(self):\n",
    "        cache_name = f\"{'-'.join(self.harvest_dir.parts)}.sqlite\"\n",
    "        Path(cache_name).unlink(missing_ok=True)\n",
    "\n",
    "    def create_dirs(self):\n",
    "        self.harvest_dir.mkdir(exist_ok=True, parents=True)\n",
    "        if self.pdf:\n",
    "            Path(self.harvest_dir, \"pdf\").mkdir(exist_ok=True)\n",
    "        if self.text:\n",
    "            Path(self.harvest_dir, \"text\").mkdir(exist_ok=True)\n",
    "        if self.image:\n",
    "            Path(self.harvest_dir, \"image\").mkdir(exist_ok=True)\n",
    "\n",
    "    def _get_total(self):\n",
    "        try:\n",
    "            params = self.query_params.copy()\n",
    "        except AttributeError:\n",
    "            raise NoQueryError\n",
    "        else:\n",
    "            params[\"n\"] = 0\n",
    "            response = self.s.get(self.api_url, params=params, headers={\"X-API-KEY\": self.key}, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            # print(response.url)\n",
    "            try:\n",
    "                results = response.json()\n",
    "            except (AttributeError, ValueError):\n",
    "                return 0\n",
    "            else:\n",
    "                return int(results[\"category\"][0][\"records\"][\"total\"])\n",
    "\n",
    "    def log_query(self):\n",
    "        \"\"\"\n",
    "        Do something with details of query -- ie log date?\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def harvest(self):\n",
    "        \"\"\"\n",
    "        Start the harvest and loop over the result set until finished.\n",
    "        \"\"\"\n",
    "        if self.total > 0:\n",
    "            params = self.query_params.copy()\n",
    "            params[\"n\"] = self.number\n",
    "            with tqdm(total=self.total, unit=\"article\") as pbar:\n",
    "                pbar.update(self.harvested)\n",
    "                while self.start and (self.harvested < self.total):\n",
    "                    params[\"s\"] = self.start\n",
    "                    response = self.s.get(self.api_url, params=params, headers={\"X-API-KEY\": self.key}, timeout=30)\n",
    "                    response.raise_for_status()\n",
    "                    # print(response.url)\n",
    "                    try:\n",
    "                        results = response.json()\n",
    "                    except (AttributeError, ValueError):\n",
    "                        # Log errors?\n",
    "                        pass\n",
    "                    else:\n",
    "                        records = results[\"category\"][0][\"records\"]\n",
    "                        self.process_results(records, pbar)\n",
    "                        # pbar.update(len(records['article']))\n",
    "        # Add the number harvested to the metadata file\n",
    "        self.update_crate()\n",
    "        self.delete_cache()\n",
    "        \n",
    "    def create_crate(self):\n",
    "        crate = ROCrate()\n",
    "        \n",
    "        # Add CreateAction with datetime started & instrument & object pointing to config\n",
    "        harvest_properties = {\n",
    "            \"@type\": \"CreateAction\",\n",
    "            \"name\": \"Run of harvester\",\n",
    "            \"startDate\": arrow.now().isoformat(),\n",
    "            \"instrument\": \"https://github.com/wragge/trove-newspaper-harvester\",\n",
    "            \"object\": \"harvester_config.json\",\n",
    "            \"actionStatus\": {\"@id\": \"http://schema.org/ActiveActionStatus\"}\n",
    "        }\n",
    "        crate.add(ContextEntity(crate, \"#harvester_run\", properties=harvest_properties))\n",
    "        \n",
    "        # Add link to action from root\n",
    "        crate.update_jsonld(\n",
    "            {\n",
    "                \"@id\": \"./\",\n",
    "                \"mainEntity\": {\"@id\": \"#harvester_run\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add harvester as software\n",
    "        harvester_properties = {\n",
    "            \"@type\": \"SoftwareApplication\",\n",
    "            \"name\": \"Trove Newspaper and Gazette Harvester\",\n",
    "            \"description\": \"The Trove Newspaper (& Gazette) Harvester makes it easy to download large quantities of digitised articles from Trove’s newspapers and gazettes.\",\n",
    "            \"documentation\": \"https://wragge.github.io/trove-newspaper-harvester/\",\n",
    "            \"url\": \"https://github.com/wragge/trove-newspaper-harvester\",\n",
    "            \"softwareVersion\": version('trove_newspaper_harvester')\n",
    "        }\n",
    "        \n",
    "        crate.add(ContextEntity(crate, \"https://github.com/wragge/trove-newspaper-harvester\", properties=harvester_properties))\n",
    "        \n",
    "        # Add config file\n",
    "        config_properties = {\n",
    "            \"@type\": \"File\",\n",
    "            \"name\": \"Trove Newspaper Harvester configuration file\",\n",
    "            \"encodingFormat\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        crate.add_file(Path(self.harvest_dir, \"harvester_config.json\"), properties=config_properties)\n",
    "        \n",
    "        # Add licences\n",
    "        # For newspaper metadata\n",
    "        nkc_properties = {\n",
    "            \"@type\": \"CreativeWork\",\n",
    "            \"url\": \"http://rightsstatements.org/vocab/NKC/1.0/\",\n",
    "            \"name\": \"No Known Copyright\",\n",
    "            \"description\": \"The organization that has made the Item available reasonably believes that the Item is not restricted by copyright or related rights, but a conclusive determination could not be made.\"\n",
    "        }\n",
    "        \n",
    "        # For text, pdfs, images\n",
    "        cne_properties = {\n",
    "            \"@type\": \"CreativeWork\",\n",
    "            \"url\": \"http://rightsstatements.org/vocab/CNE/1.0/\",\n",
    "            \"name\": \"Copyright Not Evaluated\",\n",
    "            \"description\": \"The copyright and related rights status of this Item has not been evaluated.\"\n",
    "        }\n",
    "        \n",
    "        # For crate metadata\n",
    "        cc_properties = {\n",
    "            \"name\": \"CC0 Public Domain Dedication\",\n",
    "            \"@type\": \"CreativeWork\",\n",
    "            \"url\": \"https://creativecommons.org/publicdomain/zero/1.0/\"\n",
    "        }\n",
    "        \n",
    "        for licence in [nkc_properties, cne_properties, cc_properties]:\n",
    "            crate.add(ContextEntity(crate, licence[\"url\"], properties=licence))\n",
    "        \n",
    "        # Add licence to metadata\n",
    "        crate.update_jsonld(\n",
    "            {\n",
    "                \"@id\": \"ro-crate-metadata.json\",\n",
    "                \"license\": {\"@id\": \"https://creativecommons.org/publicdomain/zero/1.0/\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        crate.write(self.harvest_dir)\n",
    "\n",
    "    def save_config(self):\n",
    "        \"\"\"\n",
    "        Save the harvester config in a JSON file.\n",
    "        Useful for documenting your harvest.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            \"query_params\": self.query_params,\n",
    "            \"key\": self.key,\n",
    "            \"full_harvest_dir\": str(self.harvest_dir),\n",
    "            \"maximum\": self.maximum,\n",
    "            \"text\": self.text,\n",
    "            \"pdf\": self.pdf,\n",
    "            \"image\": self.image,\n",
    "            \"include_linebreaks\": self.include_linebreaks,\n",
    "            #\"date_started\": arrow.utcnow().isoformat(),\n",
    "            #\"harvester\": f\"trove_newspaper_harvester v{version('trove_newspaper_harvester')}\",\n",
    "        }\n",
    "        with Path(self.harvest_dir, \"harvester_config.json\").open(\"w\") as config_file:\n",
    "            json.dump(config, config_file, indent=4)\n",
    "\n",
    "    def update_crate(self):\n",
    "        \"\"\"\n",
    "        Update the RO-Crate file with the total harvested, end date, and files.\n",
    "        \"\"\"\n",
    "        crate = ROCrate(source=self.harvest_dir)\n",
    "        \n",
    "        finished_date = arrow.now().isoformat()\n",
    "        \n",
    "        run_update = {\n",
    "            \"@id\": \"#harvester_run\",\n",
    "            \"endDate\": finished_date\n",
    "        }\n",
    "        \n",
    "        if self.harvested > 0:\n",
    "            run_update[\"actionStatus\"] = {\"@id\": \"http://schema.org/CompletedActionStatus\"}\n",
    "            run_update[\"result\"] = [{\"@id\": \"results.ndjson\"}]\n",
    "            \n",
    "            ndjson_properties = {\n",
    "                \"@type\": [\"File\", \"Dataset\"],\n",
    "                \"name\": \"Metadata of harvested articles in NDJSON format\",\n",
    "                \"dateCreated\": finished_date,\n",
    "                \"encodingFormat\": \"application/x-ndjson\",\n",
    "                \"size\": self.harvested,\n",
    "                \"contentSize\": Path(self.harvest_dir, \"results.ndjson\").stat().st_size,\n",
    "                \"license\": {\"@id\": \"http://rightsstatements.org/vocab/NKC/1.0/\"}\n",
    "            }\n",
    "\n",
    "            crate.add_file(Path(self.harvest_dir, \"results.ndjson\"), properties=ndjson_properties)\n",
    "\n",
    "            if self.text:\n",
    "                run_update[\"result\"].append({\"@id\": \"text\"})\n",
    "                text_properties = {\n",
    "                    \"@type\": [\"File\", \"Dataset\"],\n",
    "                    \"name\": \"Text files harvested from articles\",\n",
    "                    \"description\": \"There is one text file per article. The file titles include basic article metadata – the date of the article, the id number of the newspaper, and the id number of the article.\",\n",
    "                    \"dateCreated\": finished_date,\n",
    "                    \"size\": len(list(Path(self.harvest_dir, \"text\").glob('*.txt'))),\n",
    "                    \"license\": {\"@id\": \"http://rightsstatements.org/vocab/CNE/1.0/\"}\n",
    "                }\n",
    "                crate.add_file(Path(self.harvest_dir, \"text\"), properties=text_properties)\n",
    "\n",
    "            if self.pdf:\n",
    "                run_update[\"result\"].append({\"@id\": \"pdf\"})\n",
    "                pdf_properties = {\n",
    "                    \"@type\": [\"File\", \"Dataset\"],\n",
    "                    \"name\": \"PDF files of harvested articles\",\n",
    "                    \"description\": \"There is one PDF file per article. The file titles include basic article metadata – the date of the article, the id number of the newspaper, and the id number of the article.\",\n",
    "                    \"dateCreated\": finished_date,\n",
    "                    \"size\": len(list(Path(self.harvest_dir, \"pdf\").glob('*.pdf'))),\n",
    "                    \"license\": {\"@id\": \"http://rightsstatements.org/vocab/CNE/1.0/\"}\n",
    "                }\n",
    "                crate.add_file(Path(self.harvest_dir, \"pdf\"), properties=pdf_properties)\n",
    "\n",
    "            if self.image:\n",
    "                run_update[\"result\"].append({\"@id\": \"image\"})\n",
    "                image_properties = {\n",
    "                    \"@type\": [\"File\", \"Dataset\"],\n",
    "                    \"name\": \"Images of harvested articles\",\n",
    "                    \"description\": \"There can be multiple image files per article if the article was split over multiple pages. The file titles include basic article metadata – the date of the article, the id number of the newspaper, the id number of the article, and the id number of the page.\",\n",
    "                    \"dateCreated\": finished_date,\n",
    "                    \"size\": len(list(Path(self.harvest_dir, \"image\").glob('*.jpg'))),\n",
    "                    \"license\": {\"@id\": \"http://rightsstatements.org/vocab/CNE/1.0/\"}\n",
    "                }\n",
    "                crate.add_file(Path(self.harvest_dir, \"image\"), properties=image_properties)\n",
    "        else:\n",
    "            run_update[\"actionStatus\"] = {\"@id\": \"http://schema.org/FailedActionStatus\"}\n",
    "                \n",
    "        crate.update_jsonld(run_update)   \n",
    "        crate.write(self.harvest_dir)\n",
    "        \n",
    "    def add_csv_to_crate(self):\n",
    "        \"\"\"\n",
    "        Update the RO-Crate file with the total harvested, end date, and files.\n",
    "        \"\"\"\n",
    "        crate = ROCrate(source=self.harvest_dir)\n",
    "        \n",
    "        csv_properties = {\n",
    "            \"@type\": [\"File\", \"Dataset\"],\n",
    "            \"name\": \"Metadata of harvested articles in CSV format\",\n",
    "            \"dateCreated\": arrow.now().isoformat(),\n",
    "            \"encodingFormat\": \"text/csv\",\n",
    "            \"size\": self.harvested,\n",
    "            \"contentSize\": Path(self.harvest_dir, \"results.csv\").stat().st_size,\n",
    "            \"license\": {\"@id\": \"http://rightsstatements.org/vocab/NKC/1.0/\"}\n",
    "        }\n",
    "        \n",
    "        crate.add_file(Path(self.harvest_dir, \"results.csv\"), properties=csv_properties)\n",
    "        \n",
    "        crate.get(\"#harvester_run\").append_to(\"result\", {\"@id\": \"results.csv\"})\n",
    "\n",
    "        crate.write(self.harvest_dir)\n",
    "        \n",
    "    def remove_ndjson_from_crate(self):\n",
    "        crate = ROCrate(source=self.harvest_dir)\n",
    "        crate.delete(\"results.ndjson\")\n",
    "        crate.write(self.harvest_dir)\n",
    "        outputs = crate.get(\"#harvester_run\").properties()[\"result\"]\n",
    "        new_outputs = [o for o in outputs if o != {\"@id\": \"results.ndjson\"}]\n",
    "        crate.update_jsonld({\"@id\": \"#harvester_run\", \"result\": new_outputs})\n",
    "        crate.write(self.harvest_dir)\n",
    "\n",
    "    def save_csv(self):\n",
    "        \"\"\"\n",
    "        Flatten and rename data in the ndjson file to save as CSV.\n",
    "        \"\"\"\n",
    "        with Path(self.harvest_dir, \"results.csv\").open('w') as csvfile:\n",
    "            columns = ['article_id', 'title', 'date', 'page', 'newspaper_id', 'newspaper_title', 'category', 'words', 'illustrated', 'edition', 'supplement', 'section', 'url', 'page_url', 'snippet', 'relevance', 'corrections', 'last_corrected', 'tags', 'comments', 'lists', 'text', 'pdf', 'images']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            with self.ndjson_file.open(\"r\") as ndjson_file:\n",
    "                for line in ndjson_file:\n",
    "                    data = json.loads(line.strip())\n",
    "                    row = {\n",
    "                        \"article_id\": data[\"id\"],\n",
    "                        \"title\": data[\"heading\"],\n",
    "                        \"date\": data[\"date\"],\n",
    "                        \"page\": data[\"pageSequence\"],\n",
    "                        \"newspaper_id\": data[\"title\"][\"id\"],\n",
    "                        \"newspaper_title\": data[\"title\"].get(\"title\", \"\"),\n",
    "                        \"category\": data[\"category\"],\n",
    "                        \"words\": data[\"wordCount\"],\n",
    "                        \"illustrated\": data[\"illustrated\"],\n",
    "                        \"edition\": data.get(\"edition\", \"\"),\n",
    "                        \"supplement\": data.get(\"supplement\", \"\"),\n",
    "                        \"section\": data.get(\"section\", \"\"),\n",
    "                        \"url\": data[\"identifier\"],\n",
    "                        \"page_url\": data.get(\"trovePageUrl\", \"\"),\n",
    "                        \"snippet\": data.get(\"snippet\", \"\"),\n",
    "                        \"relevance\": data.get(\"relevance\", {}).get(\"score\", \"\"),\n",
    "                        \"corrections\": data.get(\"correctionCount\", 0),\n",
    "                        \"last_corrected\": data.get(\"lastCorrection\", {}).get(\"lastupdated\", \"\"),\n",
    "                        \"tags\": data.get(\"tagCount\", 0),\n",
    "                        \"comments\": data.get(\"commentCount\", 0),\n",
    "                        \"lists\": data.get(\"listCount\", 0),\n",
    "                        \"text\": data[\"articleText\"],\n",
    "                        \"pdf\": data[\"pdf\"],\n",
    "                        \"images\": \"|\".join(data[\"images\"])\n",
    "                    }\n",
    "                    writer.writerow(row)\n",
    "        self.add_csv_to_crate()\n",
    "\n",
    "    def make_filename(self, article):\n",
    "        \"\"\"\n",
    "        Create a filename for a text file or PDF.\n",
    "        For easy sorting/aggregation the filename has the format:\n",
    "            PUBLICATIONDATE-NEWSPAPERID-ARTICLEID\n",
    "        \"\"\"\n",
    "        # If the article object doesn't have basic info like date, there's something wrong\n",
    "        # Don't try and save files if that's the case\n",
    "        try:\n",
    "            date = article[\"date\"]\n",
    "        except KeyError:\n",
    "            return None\n",
    "        date = date.replace(\"-\", \"\")\n",
    "        newspaper_id = article[\"title\"][\"id\"]\n",
    "        article_id = article[\"id\"]\n",
    "        return f\"{date}-{newspaper_id}-{article_id}\"\n",
    "\n",
    "    def ping_pdf(self, ping_url):\n",
    "        \"\"\"\n",
    "        Check to see if a PDF is ready for download.\n",
    "        If a 200 status code is received, return True.\n",
    "        \"\"\"\n",
    "        ready = False\n",
    "        # req = Request(ping_url)\n",
    "        try:\n",
    "            # urlopen(req)\n",
    "            with self.s.cache_disabled():\n",
    "                response = self.s.get(ping_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "        except HTTPError:\n",
    "            if response.status_code == 423:\n",
    "                ready = False\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            ready = True\n",
    "        return ready\n",
    "\n",
    "    def get_pdf_url(self, article_id, zoom=3):\n",
    "        \"\"\"\n",
    "        Download the PDF version of an article.\n",
    "        These can take a while to generate, so we need to ping the server to see if it's ready before we download.\n",
    "        \"\"\"\n",
    "        pdf_url = None\n",
    "        # Ask for the PDF to be created\n",
    "        prep_url = \"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}/level/{}/prep\".format(\n",
    "            article_id, zoom\n",
    "        )\n",
    "        response = self.s.get(prep_url)\n",
    "        # Get the hash\n",
    "        prep_id = response.text\n",
    "        # Url to check if the PDF is ready\n",
    "        ping_url = \"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.ping?followup={}\".format(\n",
    "            article_id, zoom, prep_id\n",
    "        )\n",
    "        tries = 0\n",
    "        ready = False\n",
    "        time.sleep(1)  # Give some time to generate pdf\n",
    "        # Are you ready yet?\n",
    "        while ready is False and tries < 5:\n",
    "            ready = self.ping_pdf(ping_url)\n",
    "            if not ready:\n",
    "                tries += 1\n",
    "                time.sleep(2)\n",
    "        # Download if ready\n",
    "        if ready:\n",
    "            pdf_url = \"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{}.{}.pdf?followup={}\".format(\n",
    "                article_id, zoom, prep_id\n",
    "            )\n",
    "        return pdf_url\n",
    "\n",
    "    def get_aww_text(self, article_id):\n",
    "        # Download text using the link from the web interface\n",
    "        url = f\"https://trove.nla.gov.au/newspaper/rendition/nla.news-article{article_id}.txt\"\n",
    "        response = self.s.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "            # Remove the header\n",
    "            soup.find(\"p\").decompose()\n",
    "            soup.find(\"hr\").decompose()\n",
    "            return str(soup)\n",
    "\n",
    "    def save_text(self, article):\n",
    "        text_filename = self.make_filename(article)\n",
    "        if text_filename:\n",
    "            text_file = Path(self.harvest_dir, \"text\", f\"{text_filename}.txt\")\n",
    "            if not text_file.exists():\n",
    "                html_text = article.get(\"articleText\")\n",
    "                if not html_text:\n",
    "                    # If the text isn't in the API response (as with AWW), download separately\n",
    "                    html_text = self.get_aww_text(article[\"id\"])\n",
    "                if html_text:\n",
    "                    # Convert html to plain text\n",
    "                    text = html2text.html2text(html_text)\n",
    "                    if self.include_linebreaks == False:\n",
    "                        text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "                    with open(text_file, \"wb\") as text_output:\n",
    "                        text_output.write(text.encode(\"utf-8\"))\n",
    "                else:\n",
    "                    return \"\"\n",
    "            # Removes the output_dir from path\n",
    "            return text_file.relative_to(*text_file.parts[:2])\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def save_pdf(self, article):\n",
    "        pdf_filename = self.make_filename(article)\n",
    "        if pdf_filename:\n",
    "            pdf_file = Path(self.harvest_dir, \"pdf\", f\"{pdf_filename}.pdf\")\n",
    "            if not pdf_file.exists():\n",
    "                pdf_url = self.get_pdf_url(article[\"id\"])\n",
    "                if pdf_url:\n",
    "                    response = self.s.get(pdf_url)\n",
    "                    pdf_file.write_bytes(response.content)\n",
    "                    # Removes the output_dir from path\n",
    "                else:\n",
    "                    return \"\"\n",
    "            return pdf_file.relative_to(*pdf_file.parts[:2])\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def process_results(self, records, pbar):\n",
    "        \"\"\"\n",
    "        Processes a page full of results.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        try:\n",
    "            articles = records[\"article\"]\n",
    "        except KeyError:\n",
    "            raise\n",
    "        else:\n",
    "            with self.ndjson_file.open(\"a\") as ndjson_file:\n",
    "                for article in articles:\n",
    "                    if self.harvested >= self.total:\n",
    "                        break\n",
    "                    article_id = article[\"id\"]\n",
    "                    # rows.append(self.prepare_row(article))\n",
    "\n",
    "                    if self.pdf:\n",
    "                        pdf_file = self.save_pdf(article)\n",
    "                        article[\"pdf\"] = str(pdf_file)\n",
    "                    else:\n",
    "                        article[\"pdf\"] = \"\"\n",
    "                    if self.text:\n",
    "                        text_file = self.save_text(article)\n",
    "                        article[\"articleText\"] = str(text_file)\n",
    "                    else:\n",
    "                        article[\"articleText\"] = \"\"\n",
    "                    if self.image:\n",
    "                        images = download_images(\n",
    "                            article_id, output_dir=Path(self.harvest_dir, \"image\")\n",
    "                        )\n",
    "                        images = [str(Path(\"image\", i)) for i in images]\n",
    "                        article[\"images\"] = images\n",
    "                    else:\n",
    "                        article[\"images\"] = []\n",
    "                    ndjson_file.write(json.dumps(article) + \"\\n\")\n",
    "                    pbar.update(1)\n",
    "                    # Update the number harvested\n",
    "                    self.harvested += 1\n",
    "            # Get the nextStart token\n",
    "            try:\n",
    "                self.start = records[\"nextStart\"]\n",
    "            except KeyError:\n",
    "                self.start = None\n",
    "            # print('Harvested: {}'.format(self.harvested))\n",
    "\n",
    "\n",
    "def prepare_query(query):\n",
    "    \"\"\"\n",
    "    Converts a Trove search url into a set of parameters ready for harvesting.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * `query` [required, search url from Trove web interface or API, string]\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * a dictionary of parameters\n",
    "    \"\"\"\n",
    "    if query:\n",
    "        #if text and \"articleText\" not in query:\n",
    "            # If text is set to True, make sure the query is getting the article text\n",
    "            # Adding it here rather than to the params dict to avoid overwriting any existing include values\n",
    "            #query += \"&include=articleText\"\n",
    "        if \"api.trove.nla.gov.au\" in query:\n",
    "            # If it's an API url, no further processing of parameters needed\n",
    "            parsed_url = urlparse(query)\n",
    "            new_params = parse_qs(parsed_url.query)\n",
    "        else:\n",
    "            # These params can be accepted as is.\n",
    "            new_params = parse_query(query, 3)\n",
    "        new_params[\"encoding\"] = \"json\"\n",
    "        new_params[\"reclevel\"] = \"full\"\n",
    "        new_params[\"bulkHarvest\"] = \"true\"\n",
    "        \n",
    "        # The query parser defaults to 'newspaper,gazette' if no zone is set.\n",
    "        # But multiple zones won't work with bulkHarvest, so set to 'newspaper'.\n",
    "        #if new_params[\"zone\"] == \"newspaper,gazette\":\n",
    "        #    new_params[\"zone\"] = \"newspaper\"\n",
    "        # return '{}?{}'.format('https://api.trove.nla.gov.au/v2/result', urlencode(new_params, doseq=True))\n",
    "        return new_params\n",
    "\n",
    "\n",
    "def get_harvest(data_dir=\"data\", harvest_dir=None):\n",
    "    \"\"\"\n",
    "    Get the path to a harvest.\n",
    "    If data_dir and harvest_dir are not supplied, this will return the most recent harvest in the 'data' directory.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * `data_dir` [optional, directory for harvests, string]\n",
    "    * `harvest_dir` [optional, directory for this harvest, string]\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * a pathlib.Path object pointing to the harvest directory\n",
    "    \"\"\"\n",
    "    if harvest_dir:\n",
    "        harvest = Path(data_dir, harvest_dir)\n",
    "    else:\n",
    "        harvests = Path(\"data\").glob(\"*\")\n",
    "        harvests = sorted([d for d in harvests if d.is_dir()])\n",
    "        harvest = Path(harvests[-1])\n",
    "    return harvest\n",
    "\n",
    "\n",
    "def get_config(harvest):\n",
    "    \"\"\"\n",
    "    Get the query config parameters from a harvest directory.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * `harvest` [required, path to harvest, string or pathlib.Path]\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * config dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Path(harvest, \"harvester_config.json\").open(\"r\") as config_file:\n",
    "            config = json.load(config_file)\n",
    "    except IOError:\n",
    "        print(\"No harvest!\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def get_crate(harvest):\n",
    "    \"\"\"\n",
    "    Get the RO-Crate metadata file from a harvest directory.\n",
    "    \n",
    "     Parameters:\n",
    "\n",
    "    * `harvest` [required, path to harvest, string or pathlib.Path]\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * ROCrate object\n",
    "    \"\"\"\n",
    "    return ROCrate(source=harvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester\n",
       "\n",
       ">      Harvester (query_params=None, key=None, data_dir='data',\n",
       ">                 harvest_dir=None, config_file=None, text=False, pdf=False,\n",
       ">                 image=False, include_linebreaks=False, maximum=None)\n",
       "\n",
       "Harvest large quantities of digitised newspaper articles from Trove. Note that you must supply either `query_params` and `key` or `config_file`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `query_params` [optional, dictionary of parameters]\n",
       "* `key` [optional, Trove API key]\n",
       "* `config_file` [optional, path to a config file]\n",
       "* `data_dir` [optional, directory for harvests, string]\n",
       "* `harvest_dir` [optional, directory for this harvest, string]\n",
       "* `text` [optional, save articles as text files, True or False]\n",
       "* `pdf` [optional, save articles as PDFs, True or False]\n",
       "* `image` [optional, save articles as images, True or False]\n",
       "* `include_linebreaks` [optional, include linebreaks in text files, True or False]\n",
       "* `maximum` [optional, maximum number of results, integer]"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester\n",
       "\n",
       ">      Harvester (query_params=None, key=None, data_dir='data',\n",
       ">                 harvest_dir=None, config_file=None, text=False, pdf=False,\n",
       ">                 image=False, include_linebreaks=False, maximum=None)\n",
       "\n",
       "Harvest large quantities of digitised newspaper articles from Trove. Note that you must supply either `query_params` and `key` or `config_file`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `query_params` [optional, dictionary of parameters]\n",
       "* `key` [optional, Trove API key]\n",
       "* `config_file` [optional, path to a config file]\n",
       "* `data_dir` [optional, directory for harvests, string]\n",
       "* `harvest_dir` [optional, directory for this harvest, string]\n",
       "* `text` [optional, save articles as text files, True or False]\n",
       "* `pdf` [optional, save articles as PDFs, True or False]\n",
       "* `image` [optional, save articles as images, True or False]\n",
       "* `include_linebreaks` [optional, include linebreaks in text files, True or False]\n",
       "* `maximum` [optional, maximum number of results, integer]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Harvester)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Harvester` class configures and runs your harvest, saving results in a variety of formats.\n",
    "\n",
    "By default, the harvester will save harvests in a directory called `data`, with each individual harvest in a directory named according to the current date and time (`YYYYMMDDHHmmss` format). You can change this by setting the `data_dir` and `harvest_dir` parameters. This can help you to manage your harvests by grouping together related searches, or giving them meaningful names.\n",
    "\n",
    "The harvester generates three data files by default:\n",
    "\n",
    "* `harvester_config.json` a file that captures the parameters used to launch the harvest\n",
    "* `ro-crate-metadata.json` a metadata file documenting the harvest in [RO-Crate](https://www.researchobject.org/ro-crate/) format\n",
    "* `results.ndjson` contains details of all the harvested articles in a newline delimited JSON format (each line is a JSON object)\n",
    "\n",
    "You can convert the `ndjson` file to a CSV format using `Harvester.save_csv`.\n",
    "\n",
    "The `text`, `pdf`, and `image` options give you the option to save the contents of the articles as either text files, PDF files, or JPG images. Note that saving PDFs and images can be very slow.\n",
    "\n",
    "If you only want to harvest part of the results set you can set the `maximum` parameter to the number of records you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick start\n",
    "\n",
    "* You'll need a [Trove API key](https://trove.nla.gov.au/about/create-something/using-api) to use the harvester.\n",
    "* Just copy the url from a search in the newspapers and gazettes category.\n",
    "\n",
    "```python\n",
    "from trove_newspaper_harvester.core import prepare_query, Harvester\n",
    "\n",
    "my_api_key = \"myApIkEy\"\n",
    "search_url = \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge\"\n",
    "\n",
    "# Convert the search url into a set of API parameters\n",
    "my_query_params = prepare_query(search_url)\n",
    "\n",
    "# Initialise the Harvester\n",
    "harvester = Harvester(query_params=myquery_params, key=my_api_key)\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "```\n",
    "\n",
    "If you want to harvest the OCRd text of articles as well as metadata, add `text=True` to the harvester initialisation.\n",
    "\n",
    "```python\n",
    "# Initialise the Harvester\n",
    "harvester = Harvester(query_params=myquery_params, key=my_api_key, text=True)\n",
    "```\n",
    "\n",
    "Similarly you can harvest PDFs and images of articles by adding `pdf=True` and `image=True` to the harvester initialisation, but keep in mind that these options will make the harvest much slower!\n",
    "\n",
    "You *must* supply either `query_params` and `key`, or the path to a `config_file`. If you don't you'll get a `NoQueryError`.\n",
    "\n",
    "You can generate a set of query parameters from a Trove search url using `prepare_query()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST FOR MISSING PARAMETERS\n",
    "# You need to supply either query_params AND key, OR config_file. \n",
    "# If you don't you'll get a NoQueryError\n",
    "with ExceptionExpected(ex=NoQueryError):\n",
    "    harvester = Harvester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L604){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### prepare_query\n",
       "\n",
       ">      prepare_query (query)\n",
       "\n",
       "Converts a Trove search url into a set of parameters ready for harvesting.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `query` [required, search url from Trove web interface or API, string]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* a dictionary of parameters"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L604){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### prepare_query\n",
       "\n",
       ">      prepare_query (query)\n",
       "\n",
       "Converts a Trove search url into a set of parameters ready for harvesting.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `query` [required, search url from Trove web interface or API, string]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* a dictionary of parameters"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(prepare_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prepare_query` function converts a search url from the Trove web interface or API into a set of parameters that you can feed to `Harvester`. It uses the [trove-query-parser](https://pypi.org/project/trove-query-parser/) to do most of the work, but adds in a few extra parameters needed for the harvest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'wragge',\n",
       " 'l-state': ['New South Wales'],\n",
       " 'l-artType': 'newspapers',\n",
       " 'l-title': ['508'],\n",
       " 'l-decade': ['191'],\n",
       " 'l-category': ['Article'],\n",
       " 'category': 'newspaper',\n",
       " 'encoding': 'json',\n",
       " 'reclevel': 'full',\n",
       " 'bulkHarvest': 'true'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_params = prepare_query(\n",
    "    query=\"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-state=New%20South%20Wales&l-artType=newspapers&l-title=508&l-decade=191&l-category=Article\"\n",
    ")\n",
    "query_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST query_params()\n",
    "# Convert a url from the Trove web interface\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge\"\n",
    ")\n",
    "\n",
    "# Test the results\n",
    "assert query_params == {\n",
    "    \"q\": \"wragge\",\n",
    "    \"category\": \"newspaper\",\n",
    "    \"encoding\": \"json\",\n",
    "    \"reclevel\": \"full\",\n",
    "    \"bulkHarvest\": \"true\",\n",
    "}\n",
    "\n",
    "# Convert a url from an API request\n",
    "query_params = prepare_query(\n",
    "    \"https://api.trove.nla.gov.au/v2/result?q=wragge&category=newspaper&encoding=json&l-category=Article\"\n",
    ")\n",
    "\n",
    "assert query_params == {\n",
    "    \"q\": [\"wragge\"],\n",
    "    \"category\": [\"newspaper\"],\n",
    "    \"encoding\": \"json\",\n",
    "    \"l-category\": [\"Article\"],\n",
    "    \"reclevel\": \"full\",\n",
    "    \"bulkHarvest\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising a harvest using a `harvester_config.json` file\n",
    "\n",
    "The parameters used to initialise a harvest are saved into a file called `harvester_config.json()`. This provides useful documentation of your harvest, making it possible to reconstruct the process at a later date.\n",
    "\n",
    "For example, you might want to re-harvest a particular query a year after your initial harvest to see how the results have changed. Remember, more articles are being added every week! To re-run a harvest, just point the Harvester to the `harvester_config.json()` file. By default, your new harvest will be saved in a fresh directory.\n",
    "\n",
    "```python\n",
    "from trove_newspaper_harvester.core import Harvester\n",
    "\n",
    "harvester = Harvester(config_file=\"path/to/old/harvest/harvester_config.json\")\n",
    "\n",
    "harvester.harvest()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb65494ed26466695033437c2149240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST: Reharvest from config file\n",
    "\n",
    "API_KEY = os.getenv(\"TROVE_API_KEY\")\n",
    "\n",
    "test_config = {\n",
    "    'query_params': {'q': 'wragge',\n",
    "    'l-state': ['Western Australia'],\n",
    "    'l-illustrated': 'true',\n",
    "    'l-illtype': ['Photo'],\n",
    "    'include': ['articleText'],\n",
    "    'category': 'newspaper',\n",
    "    'encoding': 'json',\n",
    "    'reclevel': 'full',\n",
    "    'bulkHarvest': 'true'},\n",
    "    'key': API_KEY,\n",
    "    'full_harvest_dir': 'harvests/test_harvest',\n",
    "    'maximum': None,\n",
    "    'text': True,\n",
    "    'pdf': False,\n",
    "    'image': False,\n",
    "    'include_linebreaks': False\n",
    "}\n",
    "\n",
    "Path(\"harvester_config.json\").write_text(json.dumps(test_config))\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(config_file=\"harvester_config.json\")\n",
    "\n",
    "# Start the harvest!\n",
    "harvester.harvest()\n",
    "\n",
    "shutil.rmtree(Path(\"data\"))\n",
    "Path(\"harvester_config.json\").unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where your harvests are saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, harvests are saved in a directory named `data`. Each individual harvest is saved in a directory named according to the current date/time, for example: `data/20230826125205`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 140,658\n"
     ]
    }
   ],
   "source": [
    "# TEST HARVESTER CREATES DEFAULT HARVEST DIRECTORY\n",
    "# This example initialises a harvest, but doesn't actually run it.\n",
    "\n",
    "API_KEY = os.getenv(\"TROVE_API_KEY\")\n",
    "\n",
    "# Prepare query params\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge\"\n",
    ")\n",
    "\n",
    "# Initialise the Harvester with the query parameters\n",
    "harvester = Harvester(query_params=query_params, key=API_KEY, text=True)\n",
    "\n",
    "# if you haven't set the max parameter, the total value will be the total number of results\n",
    "assert harvester.total > 0\n",
    "print(f\"Total results: {harvester.total:,}\")\n",
    "\n",
    "# Check that the data directory exists\n",
    "assert Path(\"data\").exists() is True\n",
    "\n",
    "# Check that a harvest directory with the current date/hour exists in the data directory\n",
    "assert len(list(Path(\"data\").glob(f'{arrow.utcnow().format(\"YYYYMMDDHH\")}*'))) == 1\n",
    "\n",
    "# Check that a 'text' directory exists in the harvest directory\n",
    "assert (\n",
    "    Path(next(Path(\"data\").glob(f'{arrow.utcnow().format(\"YYYYMMDDHH\")}*'))).exists()\n",
    "    is True\n",
    ")\n",
    "\n",
    "# Check that the cache has been initialised\n",
    "assert Path(f\"{'-'.join(harvester.harvest_dir.parts)}.sqlite\").exists()\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(Path(\"data\"))\n",
    "harvester.delete_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the default directories using the `data_dir` and `harvest_dir` parameters. For example, if you wanted to keep all the harvests relating to a specific project together, you could set `data_dir=\"my-cool-project\"`. You can use `harvest_dir` to give your harvest a meaningful name, for example `harvest_dir=\"search-for-cat-photos\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 140,658\n"
     ]
    }
   ],
   "source": [
    "# TEST HARVESTER CREATES REQUESTED HARVEST DIRECTORY\n",
    "\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge\"\n",
    ")\n",
    "\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    data_dir=\"harvests\",\n",
    "    harvest_dir=\"my_trove_harvest\",\n",
    "    pdf=True,\n",
    "    image=True,\n",
    ")\n",
    "\n",
    "assert harvester.total > 0\n",
    "print(f\"Total results: {harvester.total:,}\")\n",
    "\n",
    "# Check that the data directory exists\n",
    "assert Path(\"harvests\").exists() is True\n",
    "\n",
    "assert Path(\"harvests\", \"my_trove_harvest\").exists() is True\n",
    "\n",
    "assert Path(\"harvests\", \"my_trove_harvest\", \"pdf\").exists() is True\n",
    "\n",
    "assert Path(\"harvests\", \"my_trove_harvest\", \"image\").exists() is True\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(Path(\"harvests\"))\n",
    "harvester.delete_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester.harvest\n",
       "\n",
       ">      Harvester.harvest ()\n",
       "\n",
       "Start the harvest and loop over the result set until finished."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester.harvest\n",
       "\n",
       ">      Harvester.harvest ()\n",
       "\n",
       "Start the harvest and loop over the result set until finished."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Harvester.harvest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the harvester is initialised, you can start the harvest by calling `Harvester.harvest()`. A progress bar will keep you informed of the status of your harvest.\n",
    "\n",
    "Add `text=True` to include the OCRd full text of the articles in the harvest. The contents of each article is saved as a separate file in the `text` directory. See the [harvest results](#harvest-results) section below for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723f9669469343bba935ec5636802363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HARVEST WITH TEXT > 100 records\n",
    "\n",
    "# Prepare query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-state=Western%20Australia&l-illustrationType=Photo\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    data_dir=\"harvests\",\n",
    "    harvest_dir=\"test_harvest\",\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "\n",
    "\n",
    "# ---TESTS---\n",
    "# Check that the ndjson file exists and lines can be parsed as json\n",
    "json_data = []\n",
    "with harvester.ndjson_file.open(\"r\") as ndjson_file:\n",
    "    for line in ndjson_file:\n",
    "        json_data.append(json.loads(line.strip()))\n",
    "\n",
    "# The length of the ndjson file should equal the number of records harvested\n",
    "assert len(json_data) == harvester.harvested\n",
    "\n",
    "# Check that the metadata file has been created\n",
    "config = get_config(harvester.harvest_dir)\n",
    "assert config[\"query_params\"] == query_params\n",
    "\n",
    "# Check that the RO-Crate file was created\n",
    "crate = get_crate(harvester.harvest_dir)\n",
    "eids = [\n",
    "    \"./\", \n",
    "    \"ro-crate-metadata.json\", \n",
    "    \"#harvester_run\", \n",
    "    \"harvester_config.json\", \n",
    "    \"https://github.com/wragge/trove-newspaper-harvester\",\n",
    "    \"results.ndjson\",\n",
    "    \"text\",\n",
    "    \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
    "    \"http://rightsstatements.org/vocab/CNE/1.0/\",\n",
    "    \"http://rightsstatements.org/vocab/NKC/1.0/\" \n",
    "]\n",
    "for eid in eids:\n",
    "    assert crate.get(eid) is not None\n",
    "\n",
    "# Check that a text file exists and can be read\n",
    "assert Path(\"harvests\", \"test_harvest\", json_data[0][\"articleText\"]).exists()\n",
    "text = Path(\"harvests\", \"test_harvest\", json_data[0][\"articleText\"]).read_text()\n",
    "assert isinstance(text, str)\n",
    "\n",
    "# Check that the cache file was deleted\n",
    "assert Path(f\"{'-'.join(harvester.harvest_dir.parts)}.sqlite\").exists() is False\n",
    "\n",
    "shutil.rmtree(Path(\"harvests\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of articles in the *Australian Women's Weekly* is not available through the API, so the harvester has to scrape it separately. This happens automatically. The code below is just a little test to make sure it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---TEST FOR AWW---\n",
    "# Prepare query params\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge\"\n",
    ")\n",
    "\n",
    "# Initialise the Harvester with the query parameters\n",
    "harvester = Harvester(query_params=query_params, key=API_KEY, text=True)\n",
    "\n",
    "# Get html text of an article\n",
    "text = harvester.get_aww_text(51187457)\n",
    "assert \"THE SHAPE OF THINGS TO COME\" in text\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(Path(\"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can include PDFs and images of the articles by adding `pdf=True` or `image=True` to the harvester initialisation. It's important to note that this will slow down the harvest a lot, as each file needs to be generated and downloaded individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d545bcef579942e8a8353a9c8cc7a642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HARVEST WITH PDF AND IMAGE -- 1 RECORD MAX\n",
    "\n",
    "# Prepare the query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-illustrationType=Cartoon\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    data_dir=\"harvests\",\n",
    "    harvest_dir=\"test_harvest\",\n",
    "    pdf=True,\n",
    "    image=True,\n",
    "    maximum=1,\n",
    ")\n",
    "\n",
    "# Start the harvest!\n",
    "harvester.harvest()\n",
    "\n",
    "\n",
    "# ---TESTS---\n",
    "\n",
    "# Check that the ndjson file exists and lines can be parsed as json\n",
    "json_data = []\n",
    "with harvester.ndjson_file.open(\"r\") as ndjson_file:\n",
    "    for line in ndjson_file:\n",
    "        json_data.append(json.loads(line.strip()))\n",
    "\n",
    "assert harvester.maximum == harvester.harvested\n",
    "\n",
    "# The length of the ndjson file should equal the number of records harvested\n",
    "assert len(json_data) == harvester.harvested\n",
    "\n",
    "# Check that a pdf and image file exist\n",
    "assert Path(\"harvests\", \"test_harvest\", json_data[0][\"pdf\"]).exists()\n",
    "assert Path(\"harvests\", \"test_harvest\", json_data[0][\"images\"][0]).exists()\n",
    "\n",
    "shutil.rmtree(Path(\"harvests\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally enough, nothing is harvested from a query with no results. Check your search and your API key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARVEST WITH NO RESULTS\n",
    "\n",
    "# Prepare query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wwgagsgshggshghso\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY\n",
    ")\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "\n",
    "assert harvester.harvested == 0\n",
    "shutil.rmtree(Path(\"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting a failed harvest\n",
    "\n",
    "The `Harvester` uses [requests-cache](https://pypi.org/project/requests-cache/) to cache API responses. This makes it easy to restart a failed harvest. All you need to do is call `Harvester.harvest()` again and it will pick up where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L395){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester.save_csv\n",
       "\n",
       ">      Harvester.save_csv ()\n",
       "\n",
       "Flatten and rename data in the ndjson file to save as CSV."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L395){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Harvester.save_csv\n",
       "\n",
       ">      Harvester.save_csv ()\n",
       "\n",
       "Flatten and rename data in the ndjson file to save as CSV."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Harvester.save_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harvested metadata is saved, by default, in a newline-delimited JSON file. If you'd prefer the results in CSV format, just call `Harvester.save_csv()`. See below for more information on results formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e25a002eb24c32819381e245774064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - save harvest results as CSV\n",
    "\n",
    "# Prepare query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-state=Western%20Australia&l-illustrationType=Photo\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    data_dir=\"harvests\",\n",
    "    harvest_dir=\"test_harvest\",\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "\n",
    "# Save results as CSV\n",
    "harvester.save_csv()\n",
    "\n",
    "# ---TESTS---\n",
    "\n",
    "# Check that CSV file exists\n",
    "csv_file = Path(harvester.harvest_dir, \"results.csv\")\n",
    "assert csv_file.exists()\n",
    "\n",
    "# Open the CSV file and check that the number of rows equals number of records harvested\n",
    "df = pd.read_csv(csv_file)\n",
    "assert df.shape[0] == harvester.harvested\n",
    "\n",
    "#shutil.rmtree(Path(\"harvests\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harvest results\n",
    "\n",
    "There will be at least two files created for each harvest:\n",
    "\n",
    "* `harvester_config.json` a file that captures the parameters used to launch the harvest\n",
    "* `ro-crate-metadata.json` a metadata file documenting the harvest in [RO-Crate](https://www.researchobject.org/ro-crate/) format\n",
    "* `results.ndjson` contains details of all the harvested articles in a newline delimited JSON format (each line is a JSON object)\n",
    "\n",
    "The `results.ndjson` stores the API results from Trove *as is*, with a couple of exceptions:\n",
    "\n",
    "* if the `text` parameter has been set to `True`, the `articleText` field will contain the path to a `.txt` file containing the OCRd text contents of the article (rather than containing the text itself)\n",
    "* similarly if PDFs and images are requests, the `pdf` and `image` fields int the `ndjson` file will point to the saved files.\n",
    "\n",
    "You'll probably find it easier to work with the results in CSV format. The `Harvester.save_csv()` method flattens the `ndjson` file and renames some columns to make them compatible with previous versions of the harvest. It produces a `results.csv` file, which is a plain text CSV (Comma Separated Values) file. You can open it with any spreadsheet program. The details recorded for each article are:\n",
    "\n",
    "* `article_id` – a unique identifier for the article\n",
    "* `title` – the title of the article\n",
    "* `date` – in ISO format, YYYY-MM-DD\n",
    "* `page` – page number (of course), but might also indicate the page is part of a supplement or special section\n",
    "* `newspaper_id` – a unique identifier for the newspaper or gazette title (this can be used to retrieve more information or build a link to the web interface)\n",
    "* `newspaper_title` – the name of the newspaper (or gazette)\n",
    "* `category` – one of ‘Article’, ‘Advertising’, ‘Detailed lists, results, guides’, ‘Family Notices’, or ‘Literature’\n",
    "* `words` – number of words in the article\n",
    "* `illustrated` – is it illustrated (values are y or n)\n",
    "* `edition` – edition of newspaper (rarely used)\n",
    "* `supplement` – section of newspaper (rarely used)\n",
    "* `section` – section of newspaper (rarely used)\n",
    "* `url` – the persistent url for the article\n",
    "* `page_url` – the persistent url of the page on which the article is published\n",
    "* `snippet` – short text sample\n",
    "* `relevance` – search relevance score of this result\n",
    "* `corrections` – number of text corrections\n",
    "* `last_correction` – date of last correction\n",
    "* `tags` – number of attached tags\n",
    "* `comments` – number of attached comments\n",
    "* `lists` – number of lists this article is included in\n",
    "* `text` – path to text file\n",
    "* `pdf` – path to PDF file\n",
    "* `image` – path to image file\n",
    "\n",
    "If you’ve asked for text files PDFs or images, there will be additional directories containing those files. Files containing the OCRd text of the articles will be saved in a directory named `text`. These are just plain text files, stripped on any HTML. These files include some basic metadata in their file titles – the date of the article, the id number of the newspaper, and the id number of the article. So, for example, the filename `19460104-1002-206680758.txt` tells you:\n",
    "\n",
    "* `19460104` – the article was published on 4 January 1946 (YYYYMMDD)\n",
    "* `1002` – the article was published in [*The Tribune*](https://trove.nla.gov.au/newspaper/title/1002)\n",
    "* `206680758` – the [article's unique identifier](http://nla.gov.au/nla.news-article206680758)\n",
    "\n",
    "As you can see, you can use the newspaper and article ids to create direct links into Trove:\n",
    "\n",
    "* to a newspaper or gazette `https://trove.nla.gov.au/newspaper/title/[newspaper id]`\n",
    "* to an article `http://nla.gov.au/nla.news-article[article id]`\n",
    "\n",
    "Similarly, if you've asked for copies of the articles as images, they'll be in a directory named `image`. The image file names are similar to the text files, but with an extra id number for the page from which the image was extracted. So, for example, the image filename `19250411-460-140772994-11900413.jpg` tells you:\n",
    "\n",
    "* `19250411` – the article was published on 11 April 1925 (YYYYMMDD)\n",
    "* `460` – the article was published in [*The Australasian*](https://trove.nla.gov.au/newspaper/title/460)\n",
    "* `140772994` – the [article's unique identifier](http://nla.gov.au/nla.news-article140772994)\n",
    "* `11900413` – the [page's unique identifier](https://trove.nla.gov.au/newspaper/page/11900413) (some articles can be split over multiple pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of articles in the *Australian Women's Weekly* is not available through the API, so the harvester has to scrape it separately. This happens automatically. The code below is just a little test to make sure it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L640){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_harvest\n",
       "\n",
       ">      get_harvest (data_dir='data', harvest_dir=None)\n",
       "\n",
       "Get the path to a harvest.\n",
       "If data_dir and harvest_dir are not supplied, this will return the most recent harvest in the 'data' directory.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `data_dir` [optional, directory for harvests, string]\n",
       "* `harvest_dir` [optional, directory for this harvest, string]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* a pathlib.Path object pointing to the harvest directory"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L640){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_harvest\n",
       "\n",
       ">      get_harvest (data_dir='data', harvest_dir=None)\n",
       "\n",
       "Get the path to a harvest.\n",
       "If data_dir and harvest_dir are not supplied, this will return the most recent harvest in the 'data' directory.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `data_dir` [optional, directory for harvests, string]\n",
       "* `harvest_dir` [optional, directory for this harvest, string]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* a pathlib.Path object pointing to the harvest directory"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_harvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/20220919200000\n"
     ]
    }
   ],
   "source": [
    "# TEST GET HARVEST\n",
    "\n",
    "# Create test folders\n",
    "Path(\"data\", \"20220919100000\").mkdir(parents=True)\n",
    "Path(\"data\", \"20220919200000\").mkdir(parents=True)\n",
    "\n",
    "# Get latest harvest folder\n",
    "harvest = get_harvest()\n",
    "print(harvest)\n",
    "\n",
    "# ---TESTS---\n",
    "assert harvest.name == \"20220919200000\"\n",
    "\n",
    "harvest = get_harvest(data_dir=\"data\", harvest_dir=\"20220919100000\")\n",
    "assert harvest.name == \"20220919100000\"\n",
    "\n",
    "shutil.rmtree(Path(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L663){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_config\n",
       "\n",
       ">      get_config (harvest)\n",
       "\n",
       "Get the query config parameters from a harvest directory.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `harvest` [required, path to harvest, string or pathlib.Path]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* config dictionary"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L663){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_config\n",
       "\n",
       ">      get_config (harvest)\n",
       "\n",
       "Get the query config parameters from a harvest directory.\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* `harvest` [required, path to harvest, string or pathlib.Path]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* config dictionary"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `harvester_config.json` file contains the parameters used to initiate a harvest. Using `get_config` you can retrieve the `harvester_config.json` for for a particular harvest. This can be useful if, for example, you want to re-run a harvest at a later data – you can just grab the `query_paramaters` and feed them into a new `Harvester` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71cfeb4ee3f4a9a9dda693efedec8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query_params': {'q': 'wragge',\n",
       "  'l-state': ['Western Australia'],\n",
       "  'l-illustrated': 'true',\n",
       "  'l-illtype': ['Photo'],\n",
       "  'category': 'newspaper',\n",
       "  'encoding': 'json',\n",
       "  'reclevel': 'full',\n",
       "  'bulkHarvest': 'true',\n",
       "  'include': ['articleText']},\n",
       " 'key': '########',\n",
       " 'full_harvest_dir': 'data/20230828053742',\n",
       " 'maximum': None,\n",
       " 'text': True,\n",
       " 'pdf': False,\n",
       " 'image': False,\n",
       " 'include_linebreaks': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-state=Western%20Australia&l-illustrationType=Photo\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "\n",
    "# Get the most recent harvest\n",
    "harvest = get_harvest()\n",
    "\n",
    "# Get the metadata\n",
    "config = get_config(harvest)\n",
    "\n",
    "# Obscure key and display\n",
    "config[\"key\"] = \"########\"\n",
    "display(config)\n",
    "\n",
    "# ---TESTS---\n",
    "assert config[\"query_params\"][\"q\"] == \"wragge\"\n",
    "assert config[\"text\"] is True\n",
    "\n",
    "shutil.rmtree(Path(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L683){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_crate\n",
       "\n",
       ">      get_crate (harvest)\n",
       "\n",
       "Get the RO-Crate metadata file from a harvest directory.\n",
       "\n",
       " Parameters:\n",
       "\n",
       "* `harvest` [required, path to harvest, string or pathlib.Path]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* ROCrate object"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L683){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_crate\n",
       "\n",
       ">      get_crate (harvest)\n",
       "\n",
       "Get the RO-Crate metadata file from a harvest directory.\n",
       "\n",
       " Parameters:\n",
       "\n",
       "* `harvest` [required, path to harvest, string or pathlib.Path]\n",
       "\n",
       "Returns:\n",
       "\n",
       "* ROCrate object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_crate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trove is changing all the time, so it's important to document your harvests. The Harvester automatically creates a metadata file using the [Research Object Crate (RO-Crate) format](https://www.researchobject.org/ro-crate/). This documents when the harvest was run, how many results were saved, and the version of the harvester. It is linked to the `harvester_config.json` file that save the query parameters and harvester settings. This function retrieves the RO-Crate file for a given harvest. It returns an RO-Crate object – see the [ro-crate.py package](https://github.com/ResearchObject/ro-crate-py) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25f9504579e48a184dcee060bceef1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro-crate-metadata.json CreativeWork\n",
      "./ Dataset\n",
      "harvester_config.json File\n",
      "results.ndjson ['File', 'Dataset']\n",
      "text ['File', 'Dataset']\n",
      "#harvester_run CreateAction\n",
      "https://github.com/wragge/trove-newspaper-harvester SoftwareApplication\n",
      "http://rightsstatements.org/vocab/NKC/1.0/ CreativeWork\n",
      "http://rightsstatements.org/vocab/CNE/1.0/ CreativeWork\n",
      "https://creativecommons.org/publicdomain/zero/1.0/ CreativeWork\n"
     ]
    }
   ],
   "source": [
    "# Prepare query parameters\n",
    "query_params = prepare_query(\n",
    "    \"https://trove.nla.gov.au/search/category/newspapers?keyword=wragge&l-state=Western%20Australia&l-illustrationType=Photo\"\n",
    ")\n",
    "\n",
    "# Initialise the harvester\n",
    "harvester = Harvester(\n",
    "    query_params=query_params,\n",
    "    key=API_KEY,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Start the harvest\n",
    "harvester.harvest()\n",
    "\n",
    "# Get the most recent harvest\n",
    "harvest = get_harvest()\n",
    "\n",
    "# Get the metadata\n",
    "crate = get_crate(harvest)\n",
    "\n",
    "for eid in crate.get_entities():\n",
    "    print(eid.id, eid.type)\n",
    "\n",
    "assert crate.get(\"./\").type == \"Dataset\"\n",
    "assert crate.get(\"harvester_config.json\").properties()[\"encodingFormat\"] == \"application/json\"\n",
    "assert crate.get(\"./\").properties()[\"mainEntity\"] == {\"@id\": \"#harvester_run\"}\n",
    "\n",
    "shutil.rmtree(Path(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NoQueryError\n",
       "\n",
       "\n",
       "\n",
       "Exception triggered by empty query."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wragge/trove-newspaper-harvester/blob/master/trove_newspaper_harvester/core.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NoQueryError\n",
       "\n",
       "\n",
       "\n",
       "Exception triggered by empty query."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NoQueryError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.net/). Support this project by becoming a [GitHub sponsor](https://github.com/sponsors/wragge?o=esb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
